{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 219108, Validation size: 27388, Test size: 27389\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"pary/hind_encorp\", trust_remote_code=True, split=\"train\")\n",
    "\n",
    "# Shuffle dataset before splitting\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Define split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
    "\n",
    "# Store in a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxValue = 0\n",
    "\n",
    "# for idxa, value in enumerate(split_datasets['train']):\n",
    "#     if len(value['translation'][SRC_LANGUAGE]) >= maxValue:\n",
    "#         maxValue = len(value['translation'][SRC_LANGUAGE])\n",
    "#     if idxa == 5:\n",
    "#         break\n",
    "        \n",
    "# print(maxValue)\n",
    "# print(len(split_datasets['train'][22]['translation']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from indicnlp.tokenize import indic_tokenize\n",
    "def english_tokenizer(text):\n",
    "    return [token.lower() for token in word_tokenize(text)]  # Tokenizes English text into words\n",
    "\n",
    "def hindi_tokenizer(text):\n",
    "    return [token for token in word_tokenize(text)]\n",
    "    # return indic_tokenize.trivial_tokenize(text, lang='hi')# Tokenizes Hindi text into words (works well for simple Hindi text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "UNK_IDX = 0\n",
    "SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "# Tokenize the dataset and add special tokens\n",
    "def tokenize_addSplTokens(sample):\n",
    "    sample['translation']['en'] = english_tokenizer(sample['translation']['en'])\n",
    "    sample['translation']['hi'] = hindi_tokenizer(sample['translation']['hi'])\n",
    "    return sample\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = split_datasets.map(tokenize_addSplTokens).remove_columns(['id', 'source', 'alignment_type', 'alignment_quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': ['enable', \"'adview\", \"'\", 'element'],\n",
       "  'hi': [\"'adview\", \"'\", 'तत्व', 'सक्षम', 'करें']}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 253, Hindi vocab size: 544\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(dataset, lang, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in dataset:\n",
    "        for word in line['translation'][lang]:\n",
    "            counter.update(word)  # Counting word occurrences\n",
    "\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items())}\n",
    "\n",
    "    # Add special tokens at the beginning\n",
    "    vocab[\"<pad>\"] = PAD_IDX\n",
    "    vocab[\"<sos>\"] = SOS_IDX\n",
    "    vocab[\"<eos>\"] = EOS_IDX\n",
    "    vocab[\"<unk>\"] = UNK_IDX\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Build vocab for English and Hindi using the train dataset\n",
    "# en_vocab = build_vocab(tokenized_datasets[\"train\"], lang=\"en\")\n",
    "# hi_vocab = build_vocab(tokenized_datasets[\"train\"], lang=\"hi\")\n",
    "en_vocab = build_vocab(tokenized_datasets[\"train\"], lang=\"en\", min_freq=2)\n",
    "hi_vocab = build_vocab(tokenized_datasets[\"train\"], lang=\"hi\", min_freq=2)\n",
    "\n",
    "print(f\"English vocab size: {len(en_vocab)}, Hindi vocab size: {len(hi_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ramayana', 'effect', 'on', 'various', 'cultures', 'and', 'civilization-', '(', 'from', 'of', 'p.d.f', ')']\n",
      "['विवध', 'संस्कृतियों', 'एवं', 'सभ्यताओं', 'पर', 'रामायण', 'का', 'प्रभाव', '-', '(', 'पी.डी.एफ़', '.', 'संरूप', 'में', ')']\n",
      "{'r': 4, 'a': 5, 'm': 6, 'y': 7, 'n': 8, 'e': 9, 'f': 10, 'c': 11, 't': 12, 'o': 13, 'v': 14, 'i': 15, 'u': 16, 's': 17, 'l': 18, 'd': 19, 'z': 20, '-': 21, '(': 22, 'p': 23, '.': 24, ')': 25, 'b': 26, \"'\": 27, 'w': 28, 'h': 29, 'j': 30, 'g': 31, 'x': 32, '1': 33, '9': 34, '2': 35, '4': 36, ',': 37, '5': 38, '3': 39, 'k': 40, 'q': 41, ':': 42, '8': 43, '7': 44, '0': 45, '/': 46, '<': 47, '>': 48, '“': 49, '”': 50, '?': 51, '%': 52, '{': 53, '}': 54, '6': 55, ';': 56, '…': 57, '_': 58, '£': 59, '[': 60, ']': 61, '&': 62, '=': 63, '!': 64, '|': 65, 'í': 66, '♫': 67, '$': 68, 'ô': 69, 'é': 70, '☻': 71, '☺': 72, 'ğ': 73, '*': 74, 'ü': 75, '+': 76, 'ć': 77, 'á': 78, 'š': 79, 'ý': 80, 'ñ': 81, '‘': 82, '¡': 83, 'ú': 84, '\\\\': 85, 'ö': 86, 'ā': 87, '@': 88, 'č': 89, 'ı': 90, 'ê': 91, 'đ': 92, 'ư': 93, 'ờ': 94, 'è': 95, 'ū': 96, '#': 97, '∶': 98, '̈': 99, 'ş': 100, 'κ': 101, 'α': 102, 'σ': 103, 'τ': 104, 'ε': 105, 'λ': 106, 'ό': 107, 'ρ': 108, 'ι': 109, 'ζ': 110, 'ο': 111, 'μ': 112, 'γ': 113, 'ί': 114, 'η': 115, 'ó': 116, '^': 117, 'ï': 118, 'ä': 119, 'à': 120, 'å': 121, '°': 122, '•': 123, 'ś': 124, 'ṛ': 125, 'ç': 126, 'ł': 127, 'ṭ': 128, 'ō': 129, 'ः': 130, '`': 131, '\\x82': 132, 'ò': 133, 'ã': 134, '©': 135, 'ø': 136, 'ə': 137, 'ż': 138, '~': 139, '€': 140, 'ʿ': 141, 'ī': 142, 'म': 143, 'े': 144, 'ं': 145, 'ह': 146, 'त': 147, '्': 148, 'व': 149, 'प': 150, 'ू': 151, 'र': 152, 'ण': 153, 'ै': 154, 'ज': 155, 'ा': 156, 'न': 157, 'ी': 158, 'ģ': 159, 'ř': 160, 'ṃ': 161, 'æ': 162, 'ë': 163, 'ő': 164, '×': 165, '¼': 166, 'ṣ': 167, 'ﬂ': 168, 'ँ': 169, '̇': 170, 'ė': 171, '½': 172, 'µ': 173, 'ě': 174, 'ṇ': 175, 'ș': 176, 'ă': 177, 'ḥ': 178, '¾': 179, 'ń': 180, '⅔': 181, '१': 182, '७': 183, '५': 184, '८': 185, 'ę': 186, 'क': 187, 'स': 188, 'थ': 189, 'ि': 190, 'ट': 191, 'द': 192, 'ल': 193, 'ए': 194, '➣': 195, 'आ': 196, 'ठ': 197, 'ब': 198, 'च': 199, 'ु': 200, 'श': 201, 'ो': 202, 'â': 203, 'ḷ': 204, 'স': 205, 'ু': 206, 'ভ': 207, 'া': 208, 'ষ': 209, 'চ': 210, 'ন': 211, '্': 212, 'দ': 213, 'র': 214, 'ব': 215, 'ख': 216, 'î': 217, 'ǆ': 218, '←': 219, 'ṅ': 220, 'ञ': 221, 'य': 222, 'औ': 223, 'ई': 224, '±': 225, 'ﬁ': 226, 'अ': 227, 'ग': 228, 'इ': 229, 'ृ': 230, 'ष': 231, 'ț': 232, 'ḍ': 233, '⌘': 234, '²': 235, 'ا': 236, 'ل': 237, 'و': 238, 'ي': 239, 'ت': 240, 'م': 241, 'ح': 242, 'د': 243, 'ة': 244, 'أ': 245, 'ر': 246, 'ك': 247, 'إ': 248, 'س': 249, '\\x1b': 250, 'ē': 251, 'ऋ': 252, '<pad>': 1, '<sos>': 2, '<eos>': 3, '<unk>': 0}\n",
      "{'व': 4, 'ि': 5, 'ध': 6, 'स': 7, 'ं': 8, '्': 9, 'क': 10, 'ृ': 11, 'त': 12, 'य': 13, 'ो': 14, 'ए': 15, 'भ': 16, 'ा': 17, 'ओ': 18, 'प': 19, 'र': 20, 'म': 21, 'ण': 22, '-': 23, '(': 24, 'ी': 25, '.': 26, 'ड': 27, 'फ': 28, '़': 29, 'ू': 30, 'े': 31, ')': 32, \"'\": 33, 'a': 34, 'd': 35, 'v': 36, 'i': 37, 'e': 38, 'w': 39, 'ष': 40, 'ज': 41, 'ह': 42, 'ल': 43, 'न': 44, 'थ': 45, 'ु': 46, 'ब': 47, 'ग': 48, 'अ': 49, 'इ': 50, 'श': 51, '।': 52, 'छ': 53, 'ँ': 54, '1': 55, '9': 56, '2': 57, '4': 58, ',': 59, 'उ': 60, 'द': 61, '5': 62, '3': 63, 'ट': 64, 'ख': 65, 'ै': 66, 'घ': 67, 'ई': 68, 'ॉ': 69, 'च': 70, 'आ': 71, '|': 72, 'S': 73, 't': 74, 'o': 75, 'c': 76, 'k': 77, 'l': 78, 'R': 79, ':': 80, '१': 81, '८': 82, '७': 83, '०': 84, '४': 85, 'औ': 86, 'A': 87, 'ठ': 88, 'ः': 89, 'झ': 90, 'I': 91, 'T': 92, '/': 93, '<': 94, '>': 95, 'ौ': 96, 's': 97, 'n': 98, '?': 99, '·': 100, 'm': 101, 'y': 102, '☻': 103, 'C': 104, 'Q': 105, 'ढ': 106, '%': 107, '{': 108, 'E': 109, 'L': 110, 'N': 111, 'U': 112, 'M': 113, 'B': 114, '}': 115, '6': 116, '३': 117, '“': 118, '”': 119, 'ऐ': 120, 'ऋ': 121, 'ऊ': 122, 'r': 123, 'f': 124, 'F': 125, '0': 126, '8': 127, '॰': 128, 'ऍ': 129, '२': 130, '…': 131, 'ञ': 132, '_': 133, 'G': 134, 'O': 135, 'H': 136, 'W': 137, '६': 138, ';': 139, 'ऑ': 140, '‘': 141, 'P': 142, 'X': 143, '५': 144, 'g': 145, 'u': 146, 'p': 147, '7': 148, 'j': 149, 'h': 150, '\\\\': 151, 'D': 152, '&': 153, 'Y': 154, '=': 155, 'V': 156, 'ॅ': 157, 'z': 158, '\\u200e': 159, 'K': 160, '!': 161, 'b': 162, 'К': 163, 'о': 164, 'р': 165, 'и': 166, 'с': 167, 'т': 168, 'у': 169, 'в': 170, 'а': 171, 'ч': 172, 'і': 173, 'д': 174, 'к': 175, 'л': 176, 'ю': 177, 'я': 178, 'Z': 179, '̈': 180, '९': 181, 'x': 182, 'н': 183, 'ф': 184, 'г': 185, 'ц': 186, '£': 187, 'ङ': 188, '̨': 189, '́': 190, 'J': 191, '̃': 192, 'Р': 193, 'з': 194, 'є': 195, '[': 196, ']': 197, 'ク': 198, 'リ': 199, 'ッ': 200, 'し': 201, 'て': 202, 'ニ': 203, 'ネ': 204, 'ー': 205, 'ム': 206, 'を': 207, '変': 208, '更': 209, 'ळ': 210, '♫': 211, '~': 212, '̧': 213, '☺': 214, 'ऎ': 215, '̂': 216, 'І': 217, 'Д': 218, 'м': 219, 'е': 220, 'ж': 221, 'п': 222, 'щ': 223, '*': 224, 'ॆ': 225, '+': 226, '$': 227, 'ハ': 228, '゙': 229, 'シ': 230, 'ョ': 231, 'ン': 232, 'П': 233, 'ь': 234, '̀': 235, '¤': 236, '\\x86': 237, 'ª': 238, '°': 239, '¥': 240, '\\x87': 241, '\\x9f': 242, 'ॊ': 243, '重': 244, '複': 245, 'ア': 246, 'ト': 247, 'レ': 248, 'ス': 249, '̌': 250, 'Ш': 251, 'ы': 252, 'ғ': 253, 'q': 254, 'Қ': 255, 'ұ': 256, 'ө': 257, 'ऒ': 258, '`': 259, 'ॠ': 260, 'ı': 261, '̆': 262, '自': 263, '動': 264, '@': 265, 'З': 266, 'ш': 267, 'б': 268, 'В': 269, 'こ': 270, 'の': 271, 'ワ': 272, '用': 273, '新': 274, 'い': 275, '゚': 276, '入': 277, '力': 278, 'く': 279, 'た': 280, 'さ': 281, '。': 282, 'ł': 283, '†': 284, '\\u200b': 285, 'А': 286, '#': 287, 'Г': 288, '̊': 289, 'Т': 290, 'С': 291, 'қ': 292, '̄': 293, '秦': 294, 'У': 295, '下': 296, 'へ': 297, '作': 298, '成': 299, 'ま': 300, 'す': 301, 'Ж': 302, 'ә': 303, 'ң': 304, 'ॄ': 305, 'И': 306, '²': 307, '\\x80': 308, '½': 309, '̇': 310, 'Б': 311, 'ъ': 312, 'х': 313, 'ॐ': 314, 'М': 315, 'Н': 316, 'コ': 317, 'マ': 318, '関': 319, '連': 320, '付': 321, 'け': 322, 'О': 323, '»': 324, '«': 325, '¿': 326, 'カ': 327, 'ウ': 328, 'と': 329, '^': 330, 'ү': 331, '\\u200c': 332, '設': 333, '定': 334, '„': 335, '通': 336, '知': 337, 'エ': 338, '表': 339, '示': 340, 'Ч': 341, '既': 342, 'に': 343, '参': 344, '加': 345, 'Е': 346, 'ऽ': 347, 'ヒ': 348, '©': 349, '接': 350, '続': 351, 'か': 352, '切': 353, 'れ': 354, 'ら': 355, 'る': 356, '削': 357, '除': 358, '失': 359, '敗': 360, '承': 361, '認': 362, '待': 363, 'ち': 364, '本': 365, '当': 366, 'メ': 367, '登': 368, '録': 369, '抹': 370, '消': 371, '？': 372, 'ॡ': 373, 'ऌ': 374, 'ラ': 375, 'フ': 376, 'ィ': 377, 'ル': 378, 'ロ': 379, '分': 380, '￼': 381, 'μ': 382, 'ا': 383, 'ل': 384, 'ٕ': 385, 'س': 386, 'م': 387, 'স': 388, 'ু': 389, 'ভ': 390, 'া': 391, 'ষ': 392, 'চ': 393, 'ন': 394, '্': 395, 'দ': 396, 'র': 397, 'ব': 398, '॔': 399, 'Ө': 400, '中': 401, '國': 402, '॑': 403, '™': 404, '離': 405, '脱': 406, 'も': 407, '、': 408, '後': 409, 'つ': 410, '再': 411, 'き': 412, '•': 413, 'オ': 414, 'イ': 415, '規': 416, 'ヘ': 417, 'そ': 418, 'は': 419, '元': 420, '戻': 421, 'せ': 422, 'な': 423, 'り': 424, '最': 425, '版': 426, 'タ': 427, '出': 428, '正': 429, 'あ': 430, 'ん': 431, 'Ы': 432, 'ت': 433, 'ج': 434, 'ح': 435, 'ق': 436, 'ر': 437, 'ٓ': 438, 'ن': 439, '送': 440, '信': 441, '名': 442, '前': 443, '順': 444, '並': 445, 'ひ': 446, '替': 447, 'え': 448, 'っ': 449, '全': 450, '含': 451, 'め': 452, '所': 453, '有': 454, '者': 455, '॥': 456, '³': 457, '利': 458, '可': 459, '能': 460, 'º': 461, '断': 462, '±': 463, '\\x14': 464, '内': 465, '小': 466, '化': 467, '開': 468, '始': 469, '手': 470, '従': 471, '¾': 472, '見': 473, '\\ufeff': 474, '選': 475, '択': 476, '状': 477, '態': 478, '拒': 479, '否': 480, '両': 481, '方': 482, '不': 483, '明': 484, '➣': 485, 'э': 486, '他': 487, '情': 488, '報': 489, '¼': 490, 'み': 491, '共': 492, '使': 493, '̣': 494, '←': 495, '短': 496, '編': 497, '集': 498, '保': 499, '存': 500, '⇒': 501, 'ী': 502, 'থ': 503, 'ঠ': 504, 'ক': 505, 'う': 506, '¬': 507, '満': 508, '員': 509, '：': 510, 'Я': 511, '求': 512, 'ァ': 513, '点': 514, 'Л': 515, '終': 516, '了': 517, '時': 518, '優': 519, '先': 520, '度': 521, 'Х': 522, 'Ł': 523, '×': 524, '⏎': 525, '\\x9c': 526, '¨': 527, '\\x95': 528, '،': 529, '越': 530, 'よ': 531, '一': 532, '致': 533, '⌘': 534, 'サ': 535, 'ホ': 536, '−': 537, '追': 538, '\\x1b': 539, '॓': 540, '打': 541, '直': 542, '未': 543, '<pad>': 1, '<sos>': 2, '<eos>': 3, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][0]['translation']['en'])\n",
    "print(tokenized_datasets['train'][0]['translation']['hi'])\n",
    "\n",
    "print(en_vocab)\n",
    "print(hi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts tokenized text into numericalized format\n",
    "def numericalize_text(text, vocab):\n",
    "    return [vocab.get(word, UNK_IDX) for word in text]\n",
    "\n",
    "\n",
    "# Adds <sos> and <eos> tokens and converts to tensor\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# Combines all transformations into one function\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Define text transformation pipeline\n",
    "text_transform = {\n",
    "    \"en\": sequential_transforms(lambda x: numericalize_text(x, en_vocab), tensor_transform),\n",
    "    \"hi\": sequential_transforms(lambda x: numericalize_text(x, hi_vocab), tensor_transform)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9895c8ff4dea4a3c8201bcb5d5b743f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb25d9755494e4684c6307d1d661b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6f548173dd433fae115850da6a72f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_sample(sample):\n",
    "    sample['translation']['en'] = text_transform[\"en\"](sample['translation']['en'])\n",
    "    sample['translation']['hi'] = text_transform[\"hi\"](sample['translation']['hi'])\n",
    "    return sample\n",
    "\n",
    "numericalized_datasets = tokenized_datasets.map(encode_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': [2, 0, 42, 3], 'hi': [2, 0, 0, 24, 73, 32, 80, 3]}}\n"
     ]
    }
   ],
   "source": [
    "print(numericalized_datasets[\"train\"][34])  # Check numericalized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        # Process and truncate source text to a max length of 1000\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))[:2000]\n",
    "        src_batch.append(processed_text)\n",
    "        src_len_batch.append(processed_text.size(0))  # Store length of source text\n",
    "\n",
    "        # Process and truncate target text to a max length of 1000\n",
    "        target_text = text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\"))[:2000]\n",
    "        trg_batch.append(target_text)\n",
    "\n",
    "    # Pad the sequences to ensure they are all the same length (max length in the batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(split_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(split_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(split_datasets['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m en, _, hi \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_batch\u001b[39m(batch):\n\u001b[0;32m      4\u001b[0m     src_batch, src_len_batch, trg_batch \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src_sample, trg_sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Process and truncate source text to a max length of 1000\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         processed_text \u001b[38;5;241m=\u001b[39m text_transform[SRC_LANGUAGE](src_sample\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))[:\u001b[38;5;241m2000\u001b[39m]\n\u001b[0;32m      8\u001b[0m         src_batch\u001b[38;5;241m.\u001b[39mappend(processed_text)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for en, _, hi in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, en\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (batch_size, seq len)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHindi shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, hi\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Hindi shape: \", hi.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Encoder and Decoder\n",
    "INPUT_DIM = len(en_vocab)\n",
    "OUTPUT_DIM = len(hi_vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "# Padding index for source and target sequences\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim   = len(en_vocab)\n",
    "output_dim  = len(hi_vocab)\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              dec_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        try:\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        \n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            try:\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(val_loader)\n",
    "test_loader_length  = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "num_epochs = 2\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    val_loss = evaluate(model, val_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample English vocab:\", list(en_vocab.items())[:20])\n",
    "print(\"Sample Hindi vocab:\", list(hi_vocab.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tokenized_datasets[\"train\"]:\n",
    "    print(\"Tokenized English:\", example[\"translation\"][\"en\"])\n",
    "    print(\"Tokenized Hindi:\", example[\"translation\"][\"hi\"])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(val_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['translation']['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample['translation']['en']).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample['translation']['hi']).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(-2) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_vocab['<unk>'] = 0\n",
    "hi_vocab['<pad>'] = 1\n",
    "hi_vocab['<sos>'] = 2\n",
    "hi_vocab['<eos>'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = list(hi_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
