{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to Hindi.\n",
    "\n",
    "## Dataset : Hind EnCorp\n",
    "HindEnCorp is a comprehensive resource for Hindi-English machine translation and bilingual NLP research, offering both aligned sentence pairs and large-scale Hindi monolingual data.\n",
    "\n",
    "## Authors\n",
    "The HindEnCorp project was led by researchers at Charles University in Prague (Czech Republic), particularly from the Institute of Formal and Applied Linguistics (UFAL). The main authors cited in the LREC 2014 publication are:\n",
    "\n",
    "- Aleš Tamchyna\n",
    "- Ondřej Bojar\n",
    "- Petr Rychlý\n",
    "- Jan Hajič"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8000, Validation size: 1000, Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"pary/hind_encorp\", trust_remote_code=True, split=\"train\").select(range(10000))\n",
    "\n",
    "# Shuffle dataset before splitting\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Define split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.1 * len(dataset))\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
    "\n",
    "# Store in a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Mara River', 'hi': 'मारा नदी'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'hi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, texts, language='en'):\n",
    "        \n",
    "        self.language = language\n",
    "        self.word2idx = {'<unk>': UNK_IDX, '<pad>': PAD_IDX, '<sos>': SOS_IDX, '<eos>': EOS_IDX}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.vocab_size = len(SPECIAL_TOKENS)\n",
    "        \n",
    "        \n",
    "        # Build vocabulary\n",
    "        word_freq = Counter()\n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 10000 == 0:\n",
    "            \n",
    "            # Apply language-specific normalization\n",
    "             if language == 'en':\n",
    "                text = text.lower()\n",
    "            \n",
    "            words = text.split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        # Add most common words to vocabulary\n",
    "        for word, freq in word_freq.most_common(50000 - len(SPECIAL_TOKENS)):\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.vocab_size\n",
    "                self.idx2word[self.vocab_size] = word\n",
    "                self.vocab_size += 1      \n",
    "    \n",
    "    def encode(self, text):\n",
    "        if self.language == 'en':\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        return [SOS_IDX] + [self.word2idx.get(word, UNK_IDX) for word in words] + [EOS_IDX]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in indices if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens\n",
    "src_texts = [sample['translation']['en'] for sample in dataset['train']]\n",
    "trg_texts = [sample['translation']['hi'] for sample in dataset['train']]\n",
    "src_tokenizer = Tokenizer(src_texts, language='en')\n",
    "trg_tokenizer = Tokenizer(trg_texts, language='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tokens: [2, 7265, 979, 3]\n",
      "Hindi tokens: [2, 1863, 412, 3]\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "\n",
    "tokens_en = src_tokenizer.encode(sample['translation']['en'])\n",
    "tokens_hi = trg_tokenizer.encode(sample['translation']['hi'])\n",
    "\n",
    "print(\"English tokens:\", tokens_en)\n",
    "print(\"Hindi tokens:\", tokens_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to text (English): mara river\n",
      "Token to text (Hindi): philosophy 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Token to text (English):\",src_tokenizer.decode(tokens_en))\n",
    "print(\"Token to text (Hindi):\",src_tokenizer.decode(tokens_hi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TranslationDataset class to function\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset_split, src_tokenizer, trg_tokenizer, max_len=2000):\n",
    "        self.examples = dataset_split\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        src_text = example['translation']['en']\n",
    "        trg_text = example['translation']['hi']\n",
    "        \n",
    "        src_tokens = self.src_tokenizer.encode(src_text)[:self.max_len]\n",
    "        trg_tokens = self.trg_tokenizer.encode(trg_text)[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(dataset['train'], src_tokenizer, trg_tokenizer)\n",
    "valid_dataset = TranslationDataset(dataset['val'], src_tokenizer, trg_tokenizer)\n",
    "test_dataset = TranslationDataset(dataset['test'], src_tokenizer, trg_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        trg_batch.append(trg_sample)\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    \n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(nn.Linear(hid_dim, pf_dim),nn.ReLU(),nn.Dropout(dropout),nn.Linear(pf_dim, hid_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device) for _ in range(n_layers) ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize layers based on attention variant\n",
    "        if attn_variant == 'multiplicative':\n",
    "            self.W = nn.Linear(self.head_dim, self.head_dim)\n",
    "        elif attn_variant == 'additive':\n",
    "            self.Wa = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.Ua = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.V = nn.Linear(self.head_dim, 1)\n",
    "        # General attention doesn't need additional parameters\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Calculate attention scores based on variant\n",
    "        if self.attn_variant == 'multiplicative':\n",
    "            # Multiplicative attention\n",
    "            K_transformed = self.W(K)\n",
    "            energy = torch.matmul(Q, K_transformed.transpose(-2, -1)) / self.scale\n",
    "            \n",
    "        elif self.attn_variant == 'general':\n",
    "            # General attention\n",
    "            energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "            \n",
    "        elif self.attn_variant == 'additive':\n",
    "            # Additive attention\n",
    "            Q_transformed = self.Wa(Q)\n",
    "            K_transformed = self.Ua(K)\n",
    "            \n",
    "            # Expand dimensions for broadcasting\n",
    "            Q_expanded = Q_transformed.unsqueeze(-2)  # [batch, heads, query_len, 1, head_dim]\n",
    "            K_expanded = K_transformed.unsqueeze(-3)  # [batch, heads, 1, key_len, head_dim]\n",
    "            \n",
    "            # Calculate additive attention\n",
    "            energy = torch.tanh(Q_expanded + K_expanded)  # [batch, heads, query_len, key_len, head_dim]\n",
    "            energy = self.V(energy).squeeze(-1)  # [batch, heads, query_len, key_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(nn.Linear(hid_dim, pf_dim),nn.ReLU(),nn.Dropout(dropout),nn.Linear(pf_dim, hid_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for src, trg in loader:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src,  trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = ['general','multiplicative',  'additive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Encoder and Decoder\n",
    "INPUT_DIM = src_tokenizer.vocab_size\n",
    "OUTPUT_DIM = trg_tokenizer.vocab_size\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "ENC_HEADS = 4\n",
    "DEC_HEADS = 4\n",
    "ENC_PF_DIM = 256\n",
    "DEC_PF_DIM = 256\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "        'Attention Type': [],\n",
    "        'Training Loss': [],\n",
    "        'Training PPL': [],\n",
    "        'Validation Loss': [],\n",
    "        'Validation PPL': [],\n",
    "        'Training Time': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention :  general\n",
      "Epoch 1 \n",
      "New best validation loss: 7.0801\n",
      "Epoch: 01\n",
      "Train Loss: 7.425 | Train PPL: 1677.404\n",
      "Val. Loss: 7.080 | Val. PPL: 1188.065\n",
      "Epoch 2 \n",
      "New best validation loss: 6.8883\n",
      "Epoch: 02\n",
      "Train Loss: 6.584 | Train PPL: 723.569\n",
      "Val. Loss: 6.888 | Val. PPL: 980.698\n",
      "Epoch 3 \n",
      "New best validation loss: 6.8189\n",
      "Epoch: 03\n",
      "Train Loss: 6.073 | Train PPL: 433.946\n",
      "Val. Loss: 6.819 | Val. PPL: 914.948\n",
      "Training Time:  884.9905552864075 \n",
      "\n",
      "\n",
      "Attention :  multiplicative\n",
      "Epoch 1 \n",
      "New best validation loss: 7.0461\n",
      "Epoch: 01\n",
      "Train Loss: 7.398 | Train PPL: 1632.610\n",
      "Val. Loss: 7.046 | Val. PPL: 1148.366\n",
      "Epoch 2 \n",
      "New best validation loss: 6.8167\n",
      "Epoch: 02\n",
      "Train Loss: 6.575 | Train PPL: 716.910\n",
      "Val. Loss: 6.817 | Val. PPL: 912.947\n",
      "Epoch 3 \n",
      "New best validation loss: 6.7694\n",
      "Epoch: 03\n",
      "Train Loss: 6.075 | Train PPL: 434.997\n",
      "Val. Loss: 6.769 | Val. PPL: 870.799\n",
      "Training Time:  928.3740427494049 \n",
      "\n",
      "\n",
      "Attention :  additive\n",
      "Epoch 1 \n",
      "New best validation loss: 6.9655\n",
      "Epoch: 01\n",
      "Train Loss: 7.402 | Train PPL: 1638.481\n",
      "Val. Loss: 6.966 | Val. PPL: 1059.484\n",
      "Epoch 2 \n",
      "New best validation loss: 6.7991\n",
      "Epoch: 02\n",
      "Train Loss: 6.456 | Train PPL: 636.212\n",
      "Val. Loss: 6.799 | Val. PPL: 897.041\n",
      "Epoch 3 \n",
      "New best validation loss: 6.7178\n",
      "Epoch: 03\n",
      "Train Loss: 5.869 | Train PPL: 353.777\n",
      "Val. Loss: 6.718 | Val. PPL: 826.957\n",
      "Training Time:  2992.822583436966 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs=3\n",
    "clip=1\n",
    "lr = 0.0005\n",
    "\n",
    "for attn_variant in attention:\n",
    "    print(\"Attention : \",attn_variant)\n",
    "    start_training_time = time.time()\n",
    "    \n",
    "    enc = Encoder(INPUT_DIM, \n",
    "                  HID_DIM, \n",
    "                  ENC_LAYERS, \n",
    "                  ENC_HEADS, \n",
    "                  ENC_PF_DIM, \n",
    "                  ENC_DROPOUT, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "    \n",
    "    dec = Decoder(OUTPUT_DIM, \n",
    "                  HID_DIM, \n",
    "                  DEC_LAYERS, \n",
    "                  DEC_HEADS, \n",
    "                  DEC_PF_DIM, \n",
    "                  DEC_DROPOUT, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "    \n",
    "    model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} \")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, len(train_loader))\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, len(valid_loader))\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            print(f\"New best validation loss: {valid_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f'model-{attn_variant}.pt')\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    training_time = time.time() - start_training_time\n",
    "    print(\"Training Time: \",training_time,\"\\n\\n\")\n",
    "    \n",
    "    model_name=f'model-{attn_variant}.pkl'\n",
    "    with open(model_name, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    # Store results\n",
    "    results['Attention Type'].append(attn_variant)\n",
    "    results['Training Loss'].append(f\"{train_losses[-1]:.3f}\")\n",
    "    results['Training PPL'].append(f\"{math.exp(train_losses[-1]):.3f}\")\n",
    "    results['Validation Loss'].append(f\"{valid_losses[-1]:.3f}\")\n",
    "    results['Validation PPL'].append(f\"{math.exp(valid_losses[-1]):.3f}\")\n",
    "    results['Training Time'].append(f\"{training_time/60:.1f}m\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.title(f'Training and Validation Losses ({attn_variant} Attention)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{attn_variant}_loss.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizers\n",
    "file_name=f'src_tokenizer.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(src_tokenizer, f)\n",
    "\n",
    "file_name=f'trg_tokenizer.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(trg_tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attention Type</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training PPL</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation PPL</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>general</td>\n",
       "      <td>6.073</td>\n",
       "      <td>433.946</td>\n",
       "      <td>6.819</td>\n",
       "      <td>914.948</td>\n",
       "      <td>884.990555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiplicative</td>\n",
       "      <td>6.075</td>\n",
       "      <td>434.997</td>\n",
       "      <td>6.769</td>\n",
       "      <td>870.799</td>\n",
       "      <td>928.374043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>additive</td>\n",
       "      <td>5.869</td>\n",
       "      <td>353.777</td>\n",
       "      <td>6.718</td>\n",
       "      <td>826.957</td>\n",
       "      <td>2992.822583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attention Type  Training Loss  Training PPL  Validation Loss  \\\n",
       "0         general          6.073       433.946            6.819   \n",
       "1  multiplicative          6.075       434.997            6.769   \n",
       "2        additive          5.869       353.777            6.718   \n",
       "\n",
       "   Validation PPL  Training Time  \n",
       "0         914.948     884.990555  \n",
       "1         870.799     928.374043  \n",
       "2         826.957    2992.822583  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Attentions**              | **Training Loss** | **Training PPL** | **Validation Loss** | **Validation PPL** |\n",
    "|-----------------------------|-------------------|------------------|---------------------|--------------------|\n",
    "| General Attention           |      6.073             |  433.946                |   6.819                 |    914.948                 |\n",
    "| Multiplicative Attention    |      6.075             |  434.997                |   6.769                  |    870.799               |\n",
    "| Additive Attention          |      5.869             |  353.777                |   6.718                  |      826.957              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "Additive Attention achieves:\n",
    "\n",
    "- Lowest Training Loss (5.869)\n",
    "- Lowest Training PPL (353.777)\n",
    "- Lowest Validation Loss (6.718)\n",
    "- Lowest Validation PPL (826.957)\n",
    "\n",
    "Multiplicative Attention is second in performance:\n",
    "\n",
    "Very close to General Attention in Training Loss and Training PPL. <br/>\n",
    "\n",
    "Slightly better than General Attention in Validation Loss (6.769 < 6.819) and Validation PPL (870.799 < 914.948).<br/>\n",
    "General Attention places third among the three in validation metrics, although its training metrics are nearly tied with Multiplicative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./loss_plots/general_loss.png\" >\n",
    "<img src = \"./loss_plots/multiplicative_loss.png\" >\n",
    "<img src = \"./loss_plots/additive_loss.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additive Attention tends to learn more expressive alignment patterns. This often translates into better capture of context, especially with languages that have complex grammar (like Hindi).\n",
    "- Multiplicative and General rely on dot-product or a linear transformation, which can be computationally cheaper but sometimes less flexible. While both can still yield good results, the alignment might be less nuanced compared to Additive Attention.\n",
    "- Because the Additive model has lower perplexity (PPL), it likely produces more coherent translations and handles vocabulary/structure alignment better. For English-Hindi translation—which can involve significant reordering of words and morphological changes—Additive’s capacity to learn more nuanced alignments is an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Efficiency\n",
    "- Multiplicative (Dot-Product) and General Attention : They are typically faster than Additive because they rely on relatively simple matrix multiplications (dot products). Fewer parameters than Additive, especially if using plain dot-product.\n",
    "- Additive Attention :Involves a feed-forward network to combine query and key vectors. This adds more learnable parameters (a small MLP) and, in some cases, can be slower.However, modern hardware (especially GPUs) handles these operations well, so the difference might not be drastic unless the dataset is very large or the model is extremely big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/server/models/src_tokenizer.pkl', 'rb') as f:\n",
    "    loaded_src_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/server/models/src_tokenizer.pkl', 'rb') as f:\n",
    "    loaded_trg_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\"How are you?\", \"आप कैसे हैं?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Convert attention to list if it's a NumPy array\n",
    "    attention = attention.tolist()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Split sentence and translation into lists of words/tokens\n",
    "    sentence_tokens = sentence.split()\n",
    "    translation_tokens = translation.split()\n",
    "    \n",
    "    # Add empty string for formatting and concatenate with tokens\n",
    "    y_ticks =  [''] + translation_tokens\n",
    "    x_ticks =  [''] + sentence_tokens\n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(model):\n",
    "    with torch.no_grad():\n",
    "        # Tokenize\n",
    "        src_tokens = torch.tensor([src_tokenizer.encode(sample[0])]).to(device)\n",
    "        trg_tokens = torch.tensor([trg_tokenizer.encode(sample[1])]).to(device)\n",
    "        \n",
    "        # Get model output and attention\n",
    "        output, attention_weights = model(src_tokens, trg_tokens[:,:-1])\n",
    "        \n",
    "        # Get last layer attention\n",
    "        if isinstance(attention_weights, list):\n",
    "            last_layer_attention = attention_weights[-1]\n",
    "        else:\n",
    "            last_layer_attention = attention_weights\n",
    "            \n",
    "        # Get first head's attention from first batch\n",
    "        attention = last_layer_attention[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Get tokens\n",
    "        src_tokens_list = src_tokenizer.encode(sample[0])\n",
    "        trg_tokens_list = trg_tokenizer.encode(sample[1])\n",
    "        \n",
    "        # Convert token IDs to text\n",
    "        src_tokens_text = [src_tokenizer.decode([token]) for token in src_tokens_list]\n",
    "        trg_tokens_text = [trg_tokenizer.decode([token]) for token in trg_tokens_list]\n",
    "        \n",
    "        # Remove special tokens\n",
    "        src_tokens_text = [t for t in src_tokens_text if t not in ['<pad>', '<sos>', '<eos>', '']]\n",
    "        trg_tokens_text = [t for t in trg_tokens_text if t not in ['<pad>', '<sos>', '<eos>', '']]\n",
    "        \n",
    "        # Display attention for general model\n",
    "        display_attention(sample[0], sample[1], attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/server/models/model-general.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_attention(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/server/models/model-multiplicative.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_attention(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/server/models/model-additive.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_attention(loaded_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
