{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\surya\\AppData\\Local\\Temp\\ipykernel_11448\\549839977.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\__init__.py\", line 629, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\functional.py\", line 6, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 2, in <module>\n",
      "    from .linear import Identity, Linear, Bilinear, LazyLinear\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 6, in <module>\n",
      "    from .. import functional as F\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\functional.py\", line 11, in <module>\n",
      "    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\_jit_internal.py\", line 26, in <module>\n",
      "    import torch.package._mangling as package_mangling\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\package\\__init__.py\", line 12, in <module>\n",
      "    from .package_importer import PackageImporter\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\package\\package_importer.py\", line 26, in <module>\n",
      "    from ._mock_zipreader import MockZipReader\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\package\\_mock_zipreader.py\", line 17, in <module>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "  File \"c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\package\\_mock_zipreader.py\", line 17, in <dictcomp>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\package\\_mock_zipreader.py:17: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:67.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "c:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.optim as optim\n",
    "import math, time\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'fr'\n",
    "\n",
    "# Load the dataset\n",
    "dataset_load = load_dataset(\"opus100\", 'en-fr', split='train[:10%]').select(range(30000))\n",
    "\n",
    "data_list = [(item['translation'][SRC_LANGUAGE], item['translation'][TRG_LANGUAGE]) for item in dataset_load]\n",
    "\n",
    "dataset = type('CustomDataset', (Dataset,), {\n",
    "    '__len__': lambda self: len(data_list),\n",
    "    '__getitem__': lambda self, idx: data_list[idx]\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"DG XVI's Internet site (electronic address: http://www.cec.lu/en/comm/dg16/dg16home.html) now contains detailed information in English on pilot projects on innovation, the Information society, new sources of employment and cultural cooperation; the application forms for these innovatory measures can be downloaded in all languages of the European Union.\",\n",
       " \"Par ailleurs, le site Internet de la DG XVI (adresse électronique: http://www.cec.lu/en/comm/dg16/dg16home.html) contient désormais une information détaillée en anglais sur les projets pilotes relatifs à la promotion de l'innovation, à la société de l'information, aux nouveaux gisements d'emploi et à la coopération culturelle; et il permet de décharger les formulaires d'inscription pour ces actions novatrices dans toutes les langues de l'Union européenne.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "dataset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = int(0.1 * len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(999)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The time now is 05:08 .', 'The time now is 05:05 .')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(dataset))\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The time now is 05:08 .\n",
      "Tokenization:  ['The', 'time', 'now', 'is', '05:08', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 16, 11, 0, 11]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'housing'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14110"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        # Process and truncate source text to a max length of 1000\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))[:2000]\n",
    "        src_batch.append(processed_text)\n",
    "        src_len_batch.append(processed_text.size(0))  # Store length of source text\n",
    "\n",
    "        # Process and truncate target text to a max length of 1000\n",
    "        target_text = text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\"))[:2000]\n",
    "        trg_batch.append(target_text)\n",
    "\n",
    "    # Pad the sequences to ensure they are all the same length (max length in the batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, de in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([32, 72])\n",
      "French shape:  torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"French shape: \", de.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(14110, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(15458, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=15458, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3612160\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "3957248\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "3957248\n",
      " 15458\n",
      "______\n",
      "15546978\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        try:\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            try:\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 11m 4s\n",
      "\tTrain Loss: 3.725 | Train PPL:  41.467\n",
      "\t Val. Loss: 3.396 |  Val. PPL:  29.837\n",
      "Epoch: 02 | Time: 11m 0s\n",
      "\tTrain Loss: 3.420 | Train PPL:  30.563\n",
      "\t Val. Loss: 3.268 |  Val. PPL:  26.269\n",
      "Epoch: 03 | Time: 11m 7s\n",
      "\tTrain Loss: 3.188 | Train PPL:  24.245\n",
      "\t Val. Loss: 3.194 |  Val. PPL:  24.378\n",
      "Epoch: 04 | Time: 11m 0s\n",
      "\tTrain Loss: 2.956 | Train PPL:  19.221\n",
      "\t Val. Loss: 3.173 |  Val. PPL:  23.872\n",
      "Epoch: 05 | Time: 10m 51s\n",
      "\tTrain Loss: 2.727 | Train PPL:  15.279\n",
      "\t Val. Loss: 3.167 |  Val. PPL:  23.730\n",
      "Epoch: 06 | Time: 10m 54s\n",
      "\tTrain Loss: 2.537 | Train PPL:  12.643\n",
      "\t Val. Loss: 3.195 |  Val. PPL:  24.401\n",
      "Epoch: 07 | Time: 10m 46s\n",
      "\tTrain Loss: 2.324 | Train PPL:  10.217\n",
      "\t Val. Loss: 3.233 |  Val. PPL:  25.361\n",
      "Epoch: 08 | Time: 10m 54s\n",
      "\tTrain Loss: 2.174 | Train PPL:   8.790\n",
      "\t Val. Loss: 3.305 |  Val. PPL:  27.259\n",
      "Epoch: 09 | Time: 10m 50s\n",
      "\tTrain Loss: 1.978 | Train PPL:   7.230\n",
      "\t Val. Loss: 3.367 |  Val. PPL:  29.004\n",
      "Epoch: 10 | Time: 10m 57s\n",
      "\tTrain Loss: 1.819 | Train PPL:   6.167\n",
      "\t Val. Loss: 3.430 |  Val. PPL:  30.884\n",
      "Epoch: 11 | Time: 10m 57s\n",
      "\tTrain Loss: 1.660 | Train PPL:   5.262\n",
      "\t Val. Loss: 3.507 |  Val. PPL:  33.361\n",
      "Epoch: 12 | Time: 10m 53s\n",
      "\tTrain Loss: 1.501 | Train PPL:   4.485\n",
      "\t Val. Loss: 3.610 |  Val. PPL:  36.969\n",
      "Epoch: 13 | Time: 10m 54s\n",
      "\tTrain Loss: 1.384 | Train PPL:   3.990\n",
      "\t Val. Loss: 3.693 |  Val. PPL:  40.168\n",
      "Epoch: 14 | Time: 10m 50s\n",
      "\tTrain Loss: 1.254 | Train PPL:   3.503\n",
      "\t Val. Loss: 3.788 |  Val. PPL:  44.165\n",
      "Epoch: 15 | Time: 10m 53s\n",
      "\tTrain Loss: 1.174 | Train PPL:   3.236\n",
      "\t Val. Loss: 3.853 |  Val. PPL:  47.138\n",
      "Epoch: 16 | Time: 10m 48s\n",
      "\tTrain Loss: 1.069 | Train PPL:   2.911\n",
      "\t Val. Loss: 3.964 |  Val. PPL:  52.681\n",
      "Epoch: 17 | Time: 10m 48s\n",
      "\tTrain Loss: 0.997 | Train PPL:   2.711\n",
      "\t Val. Loss: 4.040 |  Val. PPL:  56.801\n",
      "Epoch: 18 | Time: 10m 53s\n",
      "\tTrain Loss: 0.940 | Train PPL:   2.559\n",
      "\t Val. Loss: 4.126 |  Val. PPL:  61.954\n",
      "Epoch: 19 | Time: 10m 54s\n",
      "\tTrain Loss: 0.888 | Train PPL:   2.430\n",
      "\t Val. Loss: 4.196 |  Val. PPL:  66.393\n",
      "Epoch: 20 | Time: 10m 46s\n",
      "\tTrain Loss: 0.824 | Train PPL:   2.280\n",
      "\t Val. Loss: 4.263 |  Val. PPL:  70.998\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 20\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.256 | Test PPL:  25.949 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "with open('models/transfmodelormer.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab_transform\n",
    "with open('models/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "with open('models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(token_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/text_transform.dill', 'wb') as f:\n",
    "    dill.dump(text_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/train_loader.dill', 'wb') as f:\n",
    "    dill.dump(train_loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/test_loader.dill', 'wb') as f:\n",
    "    dill.dump(test_loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/val_loader.dill', 'wb') as f:\n",
    "    dill.dump(valid_loader, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The time now is 05:08 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The time now is 05:05 .'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  23,  90, 145,  16,   0,   6,   3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 1138, 8025, 6426, 2691,    0,    6,    3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8]), torch.Size([1, 8]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 15458])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15458])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 15458])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = test_output[1:]\n",
    "test_output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = test_output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 6426, 2691,    0,    6,    3,    0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "now\n",
      "is\n",
      "<unk>\n",
      ".\n",
      "<eos>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'The', 'time', 'now', 'is', '05:08', '.', '<eos>']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens_sur = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "test_tokens_sur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', '<unk>', 'now', 'is', '<unk>', '.', '<eos>', '<unk>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "test_trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().tolist()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Local\\Temp\\ipykernel_11448\\1777870314.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\surya\\AppData\\Local\\Temp\\ipykernel_11448\\1777870314.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(test_tokens_sur, test_trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, attention_type):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().tolist()  # Move to CPU and convert to list for plotting\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')  # Use heatmap to visualize attention matrix\n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "    \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    ax.set_title(f'Attention Mechanism: {attention_type}', fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display attention\n",
    "def display_attention(sentence, translation, attention, attention_type):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().tolist()  # Move to CPU and convert to list for plotting\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')  # Use heatmap to visualize attention matrix\n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "    \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    ax.set_title(f'Attention Mechanism: {attention_type}', fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Assuming the model has methods for each attention type:\n",
    "def display_attention_for_all_mechanisms(src_text, trg_text, model, vocab_transform, token_transform):\n",
    "    # Ensure src_text is a tensor of token indices or a list of tokens\n",
    "    if isinstance(src_text, str):\n",
    "        # If src_text is a string, tokenize it\n",
    "        src_text = token_transform[SRC_LANGUAGE](src_text)\n",
    "    \n",
    "    # If src_text is already tokenized, it is already a list of tokens\n",
    "    # Convert list of tokens to indices if it's not already a tensor of indices\n",
    "    if isinstance(src_text, list):\n",
    "        src_indices = [vocab_transform[SRC_LANGUAGE].stoi[token] for token in src_text]\n",
    "    else:\n",
    "        src_indices = src_text.tolist()  # If src_text is already a tensor, convert to list of indices\n",
    "    \n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Ensure trg_text is a tensor of token indices\n",
    "    if isinstance(trg_text, str):\n",
    "        # If trg_text is a string, tokenize it\n",
    "        trg_text = token_transform[TRG_LANGUAGE](trg_text)\n",
    "    \n",
    "    # Convert trg_text to tensor of indices using the vocabulary\n",
    "    if isinstance(trg_text, list):\n",
    "        trg_indices = [vocab_transform[TRG_LANGUAGE].stoi[token] for token in trg_text]\n",
    "    else:\n",
    "        trg_indices = trg_text.tolist()\n",
    "        \n",
    "    trg_tensor = torch.tensor(trg_indices).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output, attentions = model(src_tensor, trg_tensor)  # Turn off teacher forcing\n",
    "\n",
    "    test_output = test_output[1:]\n",
    "    output_max = test_output.argmax(1)\n",
    "    \n",
    "    # Get the mapping to convert indices to words\n",
    "    mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "    \n",
    "    # Prepare source and target tokens\n",
    "    test_tokens_sur = ['<sos>'] + src_text + ['<eos>']\n",
    "    test_trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "    \n",
    "    # General Attention\n",
    "    attention = attentions[0, 0, :, :]\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"General Attention\")\n",
    "    \n",
    "    # Multiplicative Attention (Assuming the model computes multiplicative attention)\n",
    "    attention = attentions[1, 0, :, :]\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Multiplicative Attention\")\n",
    "    \n",
    "    # Additive Attention (Assuming the model computes additive attention)\n",
    "    attention = attentions[2, 0, :, :]\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Additive Attention\")\n",
    "\n",
    "# Example usage (assuming src_text, trg_text, model, vocab_transform, and token_transform are defined)\n",
    "# Display attention for all types\n",
    "display_attention_for_all_mechanisms(src_text, trg_text, model, vocab_transform, token_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Function to compute general attention scores\n",
    "def general_attention(decoder_hidden, encoder_outputs):\n",
    "    # e_i = s^T h_i\n",
    "    attention_scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "    return attention_scores\n",
    "\n",
    "# Function to compute multiplicative attention scores\n",
    "def multiplicative_attention(decoder_hidden, encoder_outputs, W):\n",
    "    # e_i = s^T W h_i\n",
    "    decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "    attention_scores = torch.bmm(encoder_outputs, W @ decoder_hidden.transpose(1, 2)).squeeze(2)\n",
    "    return attention_scores\n",
    "\n",
    "# Function to compute additive attention scores\n",
    "def additive_attention(decoder_hidden, encoder_outputs, W1, W2, v):\n",
    "    # e_i = v^T tanh(W1 h_i + W2 s)\n",
    "    hidden_combined = torch.tanh(W1 @ encoder_outputs + W2 @ decoder_hidden)\n",
    "    attention_scores = torch.matmul(hidden_combined, v)\n",
    "    return attention_scores\n",
    "\n",
    "# Function to visualize attention\n",
    "def display_attention(sentence, translation, attention, attention_type):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().tolist()  # Move to CPU and convert to list for plotting\n",
    "    cax = ax.matshow(attention, cmap='bone')  # Use heatmap to visualize attention matrix\n",
    "\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "    y_ticks = [''] + translation\n",
    "    x_ticks = [''] + sentence\n",
    "\n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    ax.set_title(f'Attention Mechanism: {attention_type}', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Example: Using the attention functions\n",
    "def display_attention_for_all_mechanisms(src_text, trg_text, model, vocab_transform, token_transform, W, W1, W2, v):\n",
    "    # Ensure src_text is a tensor of token indices or a list of tokens\n",
    "    if isinstance(src_text, str):\n",
    "        # If src_text is a string, tokenize it\n",
    "        src_text = token_transform[SRC_LANGUAGE](src_text)\n",
    "\n",
    "    # If src_text is already tokenized, it is already a list of tokens\n",
    "    # Convert list of tokens to indices if it's not already a tensor of indices\n",
    "    if isinstance(src_text, list):\n",
    "        src_indices = [vocab_transform[SRC_LANGUAGE].get_stoi()[token] for token in src_text]\n",
    "    else:\n",
    "        src_indices = src_text.tolist()  # If src_text is already a tensor, convert to list of indices\n",
    "    \n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure trg_text is a tensor of token indices\n",
    "    if isinstance(trg_text, str):\n",
    "        # If trg_text is a string, tokenize it\n",
    "        trg_text = token_transform[TRG_LANGUAGE](trg_text)\n",
    "\n",
    "    # Convert trg_text to tensor of indices using the vocabulary\n",
    "    if isinstance(trg_text, list):\n",
    "        trg_indices = [vocab_transform[TRG_LANGUAGE].get_stoi()[token] for token in trg_text]\n",
    "    else:\n",
    "        trg_indices = trg_text.tolist()\n",
    "        \n",
    "    trg_tensor = torch.tensor(trg_indices).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output, attentions = model(src_tensor, trg_tensor)  # Turn off teacher forcing\n",
    "\n",
    "    test_output = test_output[1:]\n",
    "    output_max = test_output.argmax(1)\n",
    "\n",
    "    # Get the mapping to convert indices to words\n",
    "    mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "    # Prepare source and target tokens\n",
    "    test_tokens_sur = ['<sos>'] + src_text + ['<eos>']\n",
    "    test_trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "\n",
    "    # General Attention\n",
    "    attention = general_attention(trg_tensor, src_tensor)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"General Attention\")\n",
    "\n",
    "    # Multiplicative Attention\n",
    "    attention = multiplicative_attention(trg_tensor, src_tensor, W)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Multiplicative Attention\")\n",
    "\n",
    "    # Additive Attention\n",
    "    attention = additive_attention(trg_tensor, src_tensor, W1, W2, v)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Additive Attention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to compute general attention scores\n",
    "def general_attention(decoder_hidden, encoder_outputs):\n",
    "    # Ensure decoder_hidden has the shape [batch_size, 1, hidden_dim]\n",
    "    decoder_hidden = decoder_hidden.unsqueeze(1)  # Add an extra dimension for sequence length\n",
    "\n",
    "    # Ensure encoder_outputs has the shape [batch_size, seq_len, hidden_dim]\n",
    "    if encoder_outputs.dim() == 2:\n",
    "        # If encoder_outputs is 2D, we need to reshape it to [batch_size, seq_len, hidden_dim]\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)  # Add a dummy seq_len dimension if needed\n",
    "\n",
    "    # Perform batch matrix multiplication (bmm) to calculate attention scores\n",
    "    attention_scores = torch.bmm(encoder_outputs, decoder_hidden.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "    return attention_scores\n",
    "\n",
    "# Example: Using the attention functions\n",
    "def display_attention_for_all_mechanisms(src_text, trg_text, model, vocab_transform, token_transform, W, W1, W2, v):\n",
    "    # Ensure src_text is a tensor of token indices or a list of tokens\n",
    "    if isinstance(src_text, str):\n",
    "        # If src_text is a string, tokenize it\n",
    "        src_text = token_transform[SRC_LANGUAGE](src_text)\n",
    "\n",
    "    # If src_text is already tokenized, it is already a list of tokens\n",
    "    # Convert list of tokens to indices if it's not already a tensor of indices\n",
    "    if isinstance(src_text, list):\n",
    "        src_indices = [vocab_transform[SRC_LANGUAGE].get_stoi()[token] for token in src_text]\n",
    "    else:\n",
    "        src_indices = src_text.tolist()  # If src_text is already a tensor, convert to list of indices\n",
    "    \n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure trg_text is a tensor of token indices\n",
    "    if isinstance(trg_text, str):\n",
    "        # If trg_text is a string, tokenize it\n",
    "        trg_text = token_transform[TRG_LANGUAGE](trg_text)\n",
    "\n",
    "    # Convert trg_text to tensor of indices using the vocabulary\n",
    "    if isinstance(trg_text, list):\n",
    "        trg_indices = [vocab_transform[TRG_LANGUAGE].get_stoi()[token] for token in trg_text]\n",
    "    else:\n",
    "        trg_indices = trg_text.tolist()\n",
    "        \n",
    "    trg_tensor = torch.tensor(trg_indices).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output, attentions = model(src_tensor, trg_tensor)  # Turn off teacher forcing\n",
    "\n",
    "    test_output = test_output[1:]\n",
    "    output_max = test_output.argmax(1)\n",
    "\n",
    "    # Get the mapping to convert indices to words\n",
    "    mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "    # Prepare source and target tokens\n",
    "    test_tokens_sur = ['<sos>'] + src_text + ['<eos>']\n",
    "    test_trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "\n",
    "    # General Attention\n",
    "    attention = general_attention(trg_tensor, src_tensor)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"General Attention\")\n",
    "\n",
    "    # Multiplicative Attention\n",
    "    attention = multiplicative_attention(trg_tensor, src_tensor, W)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Multiplicative Attention\")\n",
    "\n",
    "    # Additive Attention\n",
    "    attention = additive_attention(trg_tensor, src_tensor, W1, W2, v)\n",
    "    display_attention(test_tokens_sur, test_trg_tokens, attention, \"Additive Attention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of defining a hidden_dim in your attention code\n",
    "hidden_dim = 512  # Example hidden dimension\n",
    "\n",
    "# Example of using hidden_dim for weight matrices in attention\n",
    "W = torch.rand(hidden_dim, hidden_dim)  # For multiplicative attention\n",
    "W1 = torch.rand(hidden_dim, hidden_dim)  # For additive attention\n",
    "W2 = torch.rand(hidden_dim, hidden_dim)  # For additive attention\n",
    "v = torch.rand(hidden_dim)  # For additive attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Call the function to display all types of attention\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdisplay_attention_for_all_mechanisms\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[158], line 63\u001b[0m, in \u001b[0;36mdisplay_attention_for_all_mechanisms\u001b[1;34m(src_text, trg_text, model, vocab_transform, token_transform, W, W1, W2, v)\u001b[0m\n\u001b[0;32m     60\u001b[0m test_trg_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [mapping[token\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m output_max]\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# General Attention\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mgeneral_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m display_attention(test_tokens_sur, test_trg_tokens, attention, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneral Attention\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Multiplicative Attention\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[158], line 14\u001b[0m, in \u001b[0;36mgeneral_attention\u001b[1;34m(decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a dummy seq_len dimension if needed\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Perform batch matrix multiplication (bmm) to calculate attention scores\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_scores\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Call the function to display all types of attention\n",
    "display_attention_for_all_mechanisms(src_text, trg_text, model, vocab_transform, token_transform, W, W1, W2, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('./models/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocab\n",
    "with open('./models/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "with open('./models/transfmodelormer.pkl', 'rb') as f:\n",
    "    model_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': Vocab(), 'fr': Vocab()}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': functools.partial(<function _spacy_tokenize at 0x000001F2B744AE50>, spacy=<spacy.lang.en.English object at 0x000001F28071CCA0>),\n",
       " 'fr': functools.partial(<function _spacy_tokenize at 0x000001F2B744AE50>, spacy=<spacy.lang.fr.French object at 0x000001F3008D1FD0>)}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'trg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     29\u001b[0m test_src_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m test_translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_src_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated Sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_translated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[132], line 14\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[1;34m(sentence, model_imp, tokenizer, vocab, src_lang, trg_lang, max_length)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 4: Perform inference (translation) with the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     test_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_imp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_src_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming model returns a tensor of shape (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 5: Get the predicted token indices from the model output (e.g., by taking the argmax)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m test_predicted_tokens \u001b[38;5;241m=\u001b[39m test_output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'trg'"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, model_imp, tokenizer, vocab, src_lang='en', trg_lang='fr', max_length=1000):\n",
    "    # Step 1: Tokenize the sentence using the source language tokenizer\n",
    "    test_tokenized_src = tokenizer[src_lang](sentence)\n",
    "    \n",
    "    # Step 2: Convert the tokens to indices using the source language vocab\n",
    "    test_src_indices = [vocab[src_lang].get_stoi()[token] for token in test_tokenized_src]\n",
    "    \n",
    "    # Step 3: Convert indices to a tensor and move it to the correct device\n",
    "    test_src_tensor = torch.tensor(test_src_indices).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model_imp.eval()\n",
    "    # Step 4: Perform inference (translation) with the model\n",
    "    with torch.no_grad():\n",
    "        test_output = model_imp(test_src_tensor)  # Assuming model returns a tensor of shape (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Step 5: Get the predicted token indices from the model output (e.g., by taking the argmax)\n",
    "    test_predicted_tokens = test_output.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "    # Step 6: Convert token indices to words using the target language vocab\n",
    "    test_trg_tokens = [vocab[trg_lang].itos[idx.item()] for idx in test_predicted_tokens[0]]  # Convert each token\n",
    "\n",
    "    # Step 7: Join the target tokens to form the translated sentence\n",
    "    test_translated_sentence = \" \".join(test_trg_tokens)\n",
    "\n",
    "    return test_translated_sentence\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_src_sentence = \"This is a test sentence.\"\n",
    "test_translated = translate_sentence(test_src_sentence, model_test, tokenizer, vocab)\n",
    "print(f\"Translated Sentence: {test_translated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
